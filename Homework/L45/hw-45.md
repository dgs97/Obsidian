# Практическое задание №45

**Цель:** Ознакомление с основными аспектами безопасности, сети и хранилища в Kubernetes и получение практического опыта работы с соответствующими функциями и ресурсами.

## Задание:

### Часть 1: Безопасность Kubernetes

1. **Изучите концепции безопасности Kubernetes:**
   - Авторизация
   - Аутентификация  
   - Роли
   - Механизмы контроля доступа (RBAC)
   - Прочитайте документацию Kubernetes для получения подробной информации

2. **Создайте новый сервисный аккаунт (Service Account)** в вашем кластере Kubernetes

3. **Создайте роль (Role)**, которая предоставляет разрешения только для выполнения определенных операций (например, чтение и запись конфигураций подов)

4. **Создайте привязку роли (Role Binding)**, связывающую роль с сервисным аккаунтом

5. **Создайте манифест пода**, который использует новый сервисный аккаунт
   - Убедитесь, что под может выполнять только разрешенные операции, определенные в роли

### Часть 2: Сети в Kubernetes

6. **Изучите сетевые модели в Kubernetes:**
   - ClusterIP
   - NodePort  
   - LoadBalancer

7. **Создайте новый сервис типа ClusterIP**, который обслуживает приложение в вашем кластере

8. **Создайте новый сервис типа NodePort**, который экспонирует приложение на определенном порту на каждом рабочем узле кластера

9. **Создайте новый сервис типа LoadBalancer**, который автоматически создает внешний балансировщик нагрузки для доступа к вашему приложению из внешней сети

10. **Проверьте доступность приложения**, используя созданные сервисы и их соответствующие типы доступа

### Часть 3: Хранилище в Kubernetes

11. **Изучите различные типы хранилища в Kubernetes:**
- Persistent Volumes (PV)
- Persistent Volume Claims (PVC) 
- Storage Classes

12. **Создайте Persistent Volume** с использованием локального хранилища

13. **Создайте Persistent Volume Claim**, чтобы запросить доступ к Persistent Volume, созданному в предыдущем шаге

14. **Создайте Pod**, который:
- Монтирует Persistent Volume Claim в качестве тома
- Выполняет операции чтения и записи данных

15. **Измените Storage Class** для вашего Persistent Volume, чтобы использовать другой тип хранилища (например, предоставляемый облачным провайдером)

16. **Обновите Persistent Volume Claim**, чтобы использовать обновленный Storage Class

Отлично! Давайте пройдемся по всем пунктам задания шаг за шагом. Я буду предоставлять теоретическую справку и практические примеры манифестов YAML.

---

### Часть 1: Безопасность Kubernetes (RBAC)

#### 1. Концепции безопасности (Теория)

*   **Аутентификация (Authentication):** Это процесс проверки того, кто вы. В Kubernetes это может быть сделано с помощью клиентских сертификатов, токенов (как у ServiceAccount), паролей или cloud providers. Мы будем использовать **ServiceAccount**.
*   **Авторизация (Authorization):** Это процесс проверки, имеете ли вы разрешение на выполнение определенных действий. После того как система узнает, *кто вы* (аутентификация), она проверяет, *что вам разрешено делать* (авторизация). Kubernetes использует несколько моделей авторизации, но мы сфокусируемся на **RBAC**.
*   **RBAC (Role-Based Access Control):** Механизм управления доступом, в котором права на выполнение операций назначаются не пользователям напрямую, а *ролям*. Пользователи (или сервисные аккаунты) затем получают эти роли.
    *   **Role / ClusterRole:** Определяет *правила* (например, `apiGroups`, `resources`, `verbs` like `get`, `list`, `create`, `delete`). `Role` действует в рамках конкретного namespace, `ClusterRole` — во всем кластере.
    *   **RoleBinding / ClusterRoleBinding:** Связывает субъекта (User, Group, ServiceAccount) с ролью (Role или ClusterRole). `RoleBinding` предоставляет права в рамках namespace, `ClusterRoleBinding` — во всем кластере.
*   **ServiceAccount (SA):** Аккаунт для не-человеческих пользователей, например, для Pod'ов, которым нужно взаимодействовать с API сервером Kubernetes.

#### 2. Создание ServiceAccount

Создадим файл `serviceaccount.yaml`:

```yaml
# serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: pod-config-manager
  namespace: default # Указываем namespace, в котором будем работать
```

Применим его:
```bash
kubectl apply -f serviceaccount.yaml
```

#### 3. Создание Role

Создадим роль, которая разрешает получать информацию о Pod'ах и ConfigMap'ах, а также их создавать и обновлять. Файл `role.yaml`:

```yaml
# role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: default
  name: pod-config-access
rules:
- apiGroups: [""] # "" указывает на core API group
  resources: ["pods", "pods/log"] # Разрешаем доступ к pods и их логам
  verbs: ["get", "list", "watch", "create", "update"]
- apiGroups: [""]
  resources: ["configmaps"] # Разрешаем доступ к configmaps
  verbs: ["get", "list", "create", "update"]
```

#### 4. Создание RoleBinding

Свяжем нашу роль с сервисным аккаунтом. Файл `rolebinding.yaml`:

```yaml
# rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: bind-pod-config-access
  namespace: default
subjects:
- kind: ServiceAccount
  name: pod-config-manager # Имя ServiceAccount из шага 2
  namespace: default
roleRef:
  kind: Role
  name: pod-config-access # Имя Role из шага 3
  apiGroup: rbac.authorization.k8s.io
```

Применяем Role и RoleBinding:
```bash
kubectl apply -f role.yaml
kubectl apply -f rolebinding.yaml
```

#### 5. Создание Pod с использованием ServiceAccount

Создадим Pod, который использует наш сервисный аккаунт. Для простоты возьмем образ `alpine`, который будет просто "спать". Файл `pod-with-sa.yaml`:

```yaml
# pod-with-sa.yaml
apiVersion: v1
kind: Pod
metadata:
  name: test-pod-with-sa
  namespace: default
spec:
  serviceAccountName: pod-config-manager # Важно: указываем имя созданного ServiceAccount
  containers:
  - name: alpine
    image: alpine
    command: ["/bin/sh"]
    args: ["-c", "sleep 3600"] # Контейнер просто спит
  restartPolicy: Never
```

Применим манифест:
```bash
kubectl apply -f pod-with-sa.yaml
```

**Проверка:**
Убедимся, что Pod запущен с правильным ServiceAccount.
```bash
kubectl get pod test-pod-with-sa -o yaml | grep serviceAccount
# Должно показать: serviceAccount: pod-config-manager
```

*Чтобы проверить права на практике, можно зайти в Pod и попробовать выполнить запрос к API Kubernetes (это требует установки `curl` и `jq` внутрь контейнера, что сложнее). Альтернативно, можно использовать `kubectl auth can-i` от имени service account:*
```bash
# Проверяем, может ли serviceaccount 'pod-config-manager' создавать pods в namespace 'default'
kubectl auth can-i create pods --as=system:serviceaccount:default:pod-config-manager
# Должно вернуть 'yes'

# Проверяем, может ли он удалять deployments (чего мы не разрешали)
kubectl auth can-i delete deployments --as=system:serviceaccount:default:pod-config-manager
# Должно вернуть 'no'
```

---

### Часть 2: Сети в Kubernetes

#### 6. Сетевые модели (Теория)

*   **ClusterIP:** Стандартный тип сервиса. Сервису присваивается внутренний IP-адрес, видимый только внутри кластера. Используется для связи между Pod'ами.
*   **NodePort:** Открывает *статический порт* на каждом **Worker Node**. Любой трафик, пришедший на этот порт *любого узла*, перенаправляется на сервис (и далее на Pod). Доступен извне кластера.
*   **LoadBalancer:** Создает внешний балансировщик нагрузки в облачном провайдере (AWS ELB, GCP Load Balancer и т.д.). Балансировщик автоматически направляет трафик на сервис типа NodePort (который создается автоматически). Наиболее простой способ выдать приложение наружу в облаке.

**Предположим, у нас уже есть развернутое приложение с метками `app: my-web-app` и оно слушает порт 80.**

#### 7. Сервис типа ClusterIP

Создадим файл `service-clusterip.yaml`:

```yaml
# service-clusterip.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-internal-service
spec:
  type: ClusterIP # Это значение по умолчанию, можно не указывать.
  selector:
    app: my-web-app # Указываем метки наших Pod'ов
  ports:
  - protocol: TCP
    port: 80        # Порт, на котором сервис принимает запросы
    targetPort: 80  # Порт, на который перенаправлять трафик в Pod
```

#### 8. Сервис типа NodePort

Создадим файл `service-nodeport.yaml`:

```yaml
# service-nodeport.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-nodeport-service
spec:
  type: NodePort
  selector:
    app: my-web-app
  ports:
  - protocol: TCP
    port: 80        # Внутренний порт сервиса
    targetPort: 80  # Порт Pod'а
    nodePort: 30007 # (Опционально) Фиксированный порт в диапазоне 30000-32767. Если не указать, будет выбран случайный.
```

#### 9. Сервис типа LoadBalancer

Создадим файл `service-loadbalancer.yaml`:

```yaml
# service-loadbalancer.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-external-service
spec:
  type: LoadBalancer
  selector:
    app: my-web-app
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
```

Применяем все сервисы:
```bash
kubectl apply -f service-clusterip.yaml
kubectl apply -f service-nodeport.yaml
kubectl apply -f service-loadbalancer.yaml
```

#### 10. Проверка доступности

```bash
# Посмотрим все созданные сервисы
kubectl get svc

# 1. Проверим ClusterIP. Доступен только внутри кластера.
# Для теста можно зайти в любой pod и сделать curl по DNS имени сервиса:
# curl http://my-internal-service.default.svc.cluster.local

# 2. Проверим NodePort. Узнаем внешний IP нашего узла (worker node) и порт из вывода 'kubectl get svc my-nodeport-service'
# curl http://<EXTERNAL-NODE-IP>:30007

# 3. Проверим LoadBalancer. Дождитесь, пока в столбце EXTERNAL-IP для my-external-service появится IP-адрес (это может занять время).
# curl http://<EXTERNAL-LB-IP>
```

---

### Часть 3: Хранилище в Kubernetes

#### 11. Типы хранилища (Теория)

*   **PersistentVolume (PV):** Ресурс в кластере, представляющий собой кусок готового хранилища (как физический диск в кластере). Администратор подготавливает PV.
*   **PersistentVolumeClaim (PVC):** Запрос пользователя на хранение. PVC consumes PV resources. Пользователь создает PVC, запрашивая определенный размер и режим доступа (ReadWriteOnce, ReadOnlyMany, ReadWriteMany).
*   **StorageClass (SC):** Позволяет описывать "классы" хранилища (например, "быстрое SSD", "медленный HDD"). Главная фича — **Dynamic Provisioning**. Вместо того чтобы администратору вручную создавать PV, StorageClass автоматически создаст новый PV при создании PVC.

#### 12. Создание PersistentVolume (Локальное хранилище)

**Внимание:** Локальное хранилище (`local` volume) хорошо подходит для тестов, но не является высокодоступным. Если Pod упадет и будет пересоздан на другом узле, он потеряет доступ к данным.

Создадим вручную директорию на одном из узлов (замените `<node-name>` на имя вашего worker node):
```bash
# ЗАПУСТИТЕ ЭТУ КОМАНДУ НА ВАШЕМ WORKER NODE (замените node-name)
mkdir -p /mnt/data
echo "Hello from local PV!" > /mnt/data/test.txt
```

Создадим файл `pv-local.yaml`:

```yaml
# pv-local.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: local-pv
spec:
  capacity:
    storage: 1Gi # Объем
  volumeMode: Filesystem
  accessModes:
  - ReadWriteOnce # Режим доступа: чтение-запись одним узлом
  persistentVolumeReclaimPolicy: Retain # Данные будут сохранены после удаления PVC
  storageClassName: local-storage # Имя StorageClass
  local:
    path: /mnt/data # Путь на узле, который мы создали
  nodeAffinity: # Важно! Привязываем PV к конкретному узлу
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - node-name # ЗАМЕНИТЕ на имя вашего worker node, где создана директория
```

#### 13. Создание PersistentVolumeClaim

Создадим файл `pvc-local.yaml`:

```yaml
# pvc-local.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: local-pvc
spec:
  storageClassName: local-storage # Должно совпадать с PV
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi # Запрашиваемый объем
```

Применяем PV и PVC:
```bash
kubectl apply -f pv-local.yaml
kubectl apply -f pvc-local.yaml
```
Проверим, что PVC связался с нашим PV (`STATUS` должен быть `Bound`):
```bash
kubectl get pv
kubectl get pvc
```

#### 14. Создание Pod, который использует PVC

Создадим файл `pod-with-pvc.yaml`:

```yaml
# pod-with-pvc.yaml
apiVersion: v1
kind: Pod
metadata:
  name: test-pod-with-pvc
spec:
  containers:
  - name: alpine
    image: alpine
    command: ["/bin/sh"]
    args: ["-c", "sleep 3600"]
    volumeMounts:
    - mountPath: /data # Куда монтируем внутри контейнера
      name: local-storage
  volumes:
  - name: local-storage
    persistentVolumeClaim:
      claimName: local-pvc # Имя нашего PVC
```

Применим и проверим:
```bash
kubectl apply -f pod-with-pvc.yaml
# Зайдем в pod и проверим, что видим наши данные
kubectl exec -it test-pod-with-pvc -- cat /data/test.txt
# Должно вывести: "Hello from local PV!"
# Попробуем записать что-то новое
kubectl exec -it test-pod-with-pvc -- sh -c 'echo "New data written by Pod!" >> /data/test.txt'
# Проверим на узле, что данные записались
cat /mnt/data/test.txt
```

#### 15. Изменение Storage Class

Мы не можем изменить StorageClass у уже существующего PV. Мы должны создать новый. Давайте создадим новый StorageClass (например, для облачного провайдера). Конкретный тип (`provisioner`) зависит от вашего окружения (например, `kubernetes.io/aws-ebs`, `kubernetes.io/gce-pd`).

Создадим файл `storageclass-cloud.yaml`:
```yaml
# storageclass-cloud.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: cloud-storage
provisioner: kubernetes.io/aws-ebs # Пример для AWS. ЗАМЕНИТЕ на актуальный для вашего кластера.
parameters:
  type: gp2
reclaimPolicy: Delete
volumeBindingMode: Immediate
```
Применим:
```bash
kubectl apply -f storageclass-cloud.yaml
```

#### 16. Обновление PersistentVolumeClaim

Мы не можем изменить `spec.storageClassName` у существующего PVC. Стратегия такова:
1. Удалить старый Pod, который использует PVC (`test-pod-with-pvc`).
2. Удалить старый PVC (`local-pvc`).
3. Создать *новый* PVC, который ссылается на новый StorageClass (`cloud-storage`).

Создадим файл `pvc-cloud.yaml`:

```yaml
# pvc-cloud.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: cloud-pvc # Новое имя
spec:
  storageClassName: cloud-storage # Указываем новый StorageClass
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
```

Выполним шаги:
```bash
kubectl delete pod test-pod-with-pvc
kubectl delete pvc local-pvc
# (Опционально) Если PV был с политикой Retain, его тоже нужно удалить вручную
# kubectl delete pv local-pv

kubectl apply -f pvc-cloud.yaml
```
Теперь, если в вашем кластере настроен Dynamic Provisioning, Kubernetes автоматически создаст новый PV (например, в вашем облаке) и свяжет его с PVC `cloud-pvc`. После этого вы можете создать Pod, который использует `claimName: cloud-pvc`.

---

Это полное практическое руководство по заданию. Вы можете применять манифесты по очереди, наблюдая за результатами команд `kubectl get`. Удачи